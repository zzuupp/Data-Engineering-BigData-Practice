{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM5lqEzqQuBqMDf9MvhS1v1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zzuupp/Data-Engineering-BigData-Practice/blob/main/Spark/Spark_Beginner_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spark : 초보자용 튜토리얼"
      ],
      "metadata": {
        "id": "PqmluqZBbeq0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "> 아이리스 데이터를 이용하여, 스파크를 어떻게 사용하는지 살펴보기."
      ],
      "metadata": {
        "id": "_JDC7nvOb0Dy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 라이브러리 가져오기\n",
        "* 스파크 세션 구축\n",
        "* 데이터 로드\n",
        "* 데이터 탐색 및 준비\n",
        "* 피처 엔지니어링\n",
        "* 데이터 스케일링\n",
        "* 데이터 분할\n",
        "* 모델 구축, 훈련 및 평가"
      ],
      "metadata": {
        "id": "MoZLtkZubqUy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "install Apache Spark"
      ],
      "metadata": {
        "id": "PtiWmKLkbzqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install pyspark"
      ],
      "metadata": {
        "id": "EQFd2IqKcDlU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ],
      "metadata": {
        "id": "9Ou3OwWLcG3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "#Apache Spark Libraries\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Apache Spark ML CLassifier Libraries\n",
        "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier, NaiveBayes\n",
        "\n",
        "# Apache Spark Evaluation Library\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Apache Spark Feature libraries\n",
        "from pyspark.ml.feature import StandardScaler, StringIndexer, VectorAssembler, VectorIndexer, OneHotEncoder\n",
        "\n",
        "# Apache Spark 'DenseVector'\n",
        "from pyspark.ml.linalg import DenseVector\n",
        "\n",
        "# Data Split Libraries\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Tabulating Data\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Garbage\n",
        "import gc"
      ],
      "metadata": {
        "id": "gMbnodeFcOWr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Spark Session"
      ],
      "metadata": {
        "id": "6xrG2Rp6hLss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Building Spark Session : 스파크 앱을 사용하기 위한 진입점.\n",
        "# 내가 사용할 spark의 이름 지정.\n",
        "# 'spark.executor.memory' : 실행 시, 각각의 작업 프로세스 (Executor)에 할당 메모리 크기 설정.\n",
        "#                           클러스터 모드에서는 여러 Executor가 뜨기 때문에 이 설정이 중요.\n",
        "\n",
        "# \"spark.excutor.cores\" : Executor가 사용하는 CPU 코어 수를 설정\n",
        "\n",
        "# .getOrCreate() : 같은 이름의 SparkSession 이 있다면 그걸 가져오고, 없으면 새로 만들어줘!\n",
        "\n",
        "# SparkSession : 고수준 API(DataFrame SQL) 등을 다룸. *사용자가 이해하기 쉽게 추상화된 인터페이스\n",
        "spark = (SparkSession.builder\n",
        "                            .appName('Apach Spark Beginner Tutorial')\n",
        "                            .config('spark.executor.memory', '1G')\n",
        "                            .config(\"spark.executor.cores\", '4')\n",
        "                            .getOrCreate())"
      ],
      "metadata": {
        "id": "6cu3eMKdnID1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sparkContext : 스파크 엔진과 직접 연결된 컨트롤 객체\n",
        "# .setLogLevel : 로그 출력 수준을 설정하는 함수\n",
        "#               ('INFO') → 로그 수준을 INFO로 맞춰라 (많은 실행 정보가 보임)\n",
        "spark.sparkContext.setLogLevel('INFO')"
      ],
      "metadata": {
        "id": "fbzvLJ_3pMFN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Load"
      ],
      "metadata": {
        "id": "uoQSV2Svvb-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O iris.csv https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8leXCLE3FKn",
        "outputId": "ece970ae-203b-4476-d9c4-66290dcd340c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-29 06:56:27--  https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3716 (3.6K) [text/plain]\n",
            "Saving to: ‘iris.csv’\n",
            "\n",
            "\riris.csv              0%[                    ]       0  --.-KB/s               \riris.csv            100%[===================>]   3.63K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-09-29 06:56:27 (44.5 MB/s) - ‘iris.csv’ saved [3716/3716]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# .option('header', 'true') : 첫줄을 컬럼명으로 사용(없다면 c0, c1...이런식임)\n",
        "# .option('inferSchema', 'true') \\ # 문자열이 아닌 숫자 / 실수 등을 자동으로 타입 추론.\n",
        "data = (spark.read.format(\"csv\")\n",
        "        .option('header', 'true')\n",
        "        .option('inferSchema', 'true')\n",
        "        .load('iris.csv'))\n",
        "\n",
        "# .cache() → 데이터를 메모리에 올려두고 이후 연산 시 재사용 (속도 ↑)\n",
        "data.cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ah8DaHW7vg0O",
        "outputId": "bde6dd62-a0de-4d1b-84f8-92938e8a7f8c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[sepal_length: double, sepal_width: double, petal_length: double, petal_width: double, species: string]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Exploration & Preparation"
      ],
      "metadata": {
        "id": "HQZ-0HoN2I7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XU3J5jT3gx0",
        "outputId": "a1ab0742-ab3d-47f6-a7de-87caa887455e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "150"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Type\n",
        "data.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NW1kA1V27DaN",
        "outputId": "4c6d8254-5981-4452-cb19-38ed81ed37fb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- sepal_length: double (nullable = true)\n",
            " |-- sepal_width: double (nullable = true)\n",
            " |-- petal_length: double (nullable = true)\n",
            " |-- petal_width: double (nullable = true)\n",
            " |-- species: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Display recodes\n",
        "data.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XK7btAtt77rN",
        "outputId": "2195a53b-4c45-41ae-f5d5-a4ad5c4c0d72"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----------+------------+-----------+-------+\n",
            "|sepal_length|sepal_width|petal_length|petal_width|species|\n",
            "+------------+-----------+------------+-----------+-------+\n",
            "|         5.1|        3.5|         1.4|        0.2| setosa|\n",
            "|         4.9|        3.0|         1.4|        0.2| setosa|\n",
            "|         4.7|        3.2|         1.3|        0.2| setosa|\n",
            "|         4.6|        3.1|         1.5|        0.2| setosa|\n",
            "|         5.0|        3.6|         1.4|        0.2| setosa|\n",
            "+------------+-----------+------------+-----------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Record per Species\n",
        "data.groupBy('species').count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11rZCXju8QrG",
        "outputId": "78bf580c-16d8-45be-f04c-3def0a066db3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+\n",
            "|   species|count|\n",
            "+----------+-----+\n",
            "| virginica|   50|\n",
            "|versicolor|   50|\n",
            "|    setosa|   50|\n",
            "+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Summary Stats\n",
        "\n",
        "# stddev : 표준편차.\n",
        "#     ex : sepal_length의 평균 값은 5.84cm인데 ± 0.828 범위내에 분포\n",
        "\n",
        "data.describe().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzrvvbAl8Zr-",
        "outputId": "0255905c-455e-4610-eaa7-4e82351fda8a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------+-------------------+------------------+------------------+---------+\n",
            "|summary|      sepal_length|        sepal_width|      petal_length|       petal_width|  species|\n",
            "+-------+------------------+-------------------+------------------+------------------+---------+\n",
            "|  count|               150|                150|               150|               150|      150|\n",
            "|   mean| 5.843333333333335| 3.0540000000000007|3.7586666666666693|1.1986666666666672|     NULL|\n",
            "| stddev|0.8280661279778637|0.43359431136217375| 1.764420419952262|0.7631607417008414|     NULL|\n",
            "|    min|               4.3|                2.0|               1.0|               0.1|   setosa|\n",
            "|    max|               7.9|                4.4|               6.9|               2.5|virginica|\n",
            "+-------+------------------+-------------------+------------------+------------------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "예측을 시작하기 위해, Species 즉 Label 컬럼은 숫자 값이어야 한다 (모델은 문자열을 싫어한다!).\n",
        "\n",
        "\n",
        "이를 달성하기 위해 우리는 Species 컬럼에 문자열 인덱싱(String Indexing) 을 적용할 것이다."
      ],
      "metadata": {
        "id": "NUNEKKkb8iAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#String Indexing the Species column\n",
        "SIndexer = StringIndexer(inputCol='species', outputCol = 'species_indx')\n",
        "\n",
        "# .fit(data) 위에서 지정 규칙을 학습만 함 (아직 변환 x)\n",
        "# .transform(data) = 변환 규칙을 적용해서 새 컬럼 생성 (하지만 진짜 계산은 Action 때 실행).\n",
        "# 즉 실제 계산은 이 시점에서도 안일어남, **실행 계획(Execution Plan)**을 DAG(Directed Acyclic Graph)로 저장한 것.\n",
        "data = SIndexer.fit(data).transform(data)\n",
        "\n",
        "# 비로소 실행.\n",
        "data.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9mFxcvt9So3",
        "outputId": "9537f9d8-e1ce-49a2-bffe-715e01a04bb1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----------+------------+-----------+-------+------------+\n",
            "|sepal_length|sepal_width|petal_length|petal_width|species|species_indx|\n",
            "+------------+-----------+------------+-----------+-------+------------+\n",
            "|         5.1|        3.5|         1.4|        0.2| setosa|         0.0|\n",
            "|         4.9|        3.0|         1.4|        0.2| setosa|         0.0|\n",
            "|         4.7|        3.2|         1.3|        0.2| setosa|         0.0|\n",
            "|         4.6|        3.1|         1.5|        0.2| setosa|         0.0|\n",
            "|         5.0|        3.6|         1.4|        0.2| setosa|         0.0|\n",
            "+------------+-----------+------------+-----------+-------+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "즉, **Spark**는 **스키마 기반 (SQL 테이블 느낌)**, Pandas는 딕셔너리 기반 (유연)이라 생긴 차이"
      ],
      "metadata": {
        "id": "kwrwvM3i_4YU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OqkxA4pr_7Sr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Engineering"
      ],
      "metadata": {
        "id": "eMgqcElq9yD0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "The Spark model needs two columns: “label” and “features” and we are not going to do much feature engineering because we want to focus on the mechanics of training the model in Spark. So, creating a seperate dataframe with re-ordered columns, then defining an input data using Dense Vector. A Dense Vector is a local vector that is backed by a double array that represents its entry values. In other words, it's used to store arrays of values for use in PySpark.\n",
        "\n",
        "\n",
        "\n",
        "스파크에서 모델을 학습하려면 “label” 과 “features” 두 컬럼이 꼭 필요하다.\n",
        "\n",
        "이번에는 모델 학습 과정을 익히는 데 집중할 거라 복잡한 피처 엔지니어링은 따로 하지 않는다.\n",
        "\n",
        "그래서 컬럼 순서를 재배치하여 별도의 데이터프레임을 생성 후, 입력 데이터는 Dense Vector라는 형태로 정의.\n",
        "\n",
        "Dense Vector는 내부적으로 double 배열을 기반으로 값을 담는 로컬 벡터인데,\n",
        "\n",
        "쉽게 말해 PySpark에서 여러 값들을 묶어 저장하고 모델에 넣을 때 쓰이는 자료 구조라고 보면 된다"
      ],
      "metadata": {
        "id": "boIOxPF6DGO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a seperate dataframe with re-ordered columns\n",
        "df = data.select('species_indx', 'sepal_length', 'sepal_width', 'petal_length', 'petal_width')\n",
        "\n",
        "# Inspect the dataframe\n",
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqWQJSTn_7Ah",
        "outputId": "b4422f9e-7125-4232-de15-737ea71dffed"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+------------+-----------+------------+-----------+\n",
            "|species_indx|sepal_length|sepal_width|petal_length|petal_width|\n",
            "+------------+------------+-----------+------------+-----------+\n",
            "|         0.0|         5.1|        3.5|         1.4|        0.2|\n",
            "|         0.0|         4.9|        3.0|         1.4|        0.2|\n",
            "|         0.0|         4.7|        3.2|         1.3|        0.2|\n",
            "|         0.0|         4.6|        3.1|         1.5|        0.2|\n",
            "|         0.0|         5.0|        3.6|         1.4|        0.2|\n",
            "+------------+------------+-----------+------------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: Observe that the species column which is our label (aka Target) is now at beginning of the dataframe\n",
        "\n"
      ],
      "metadata": {
        "id": "5POtopVWGXBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the `input_data` as Dense Vector\n",
        "input_data = df.rdd.map(lambda x: (x[0], DenseVector(x[1:])))\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "***\n",
        "🔎 DenseVector란?\n",
        "Spark ML에서 쓰이는 특수한 벡터 자료구조.\n",
        "내부적으로 double 배열 기반.\n",
        "보통 features 컬럼이 DenseVector 형태여야 모델에 넣을 수 있어요\n",
        "\n",
        "\n",
        "***\n",
        "from pyspark.ml.linalg import DenseVector\n",
        "\n",
        "v = DenseVector([1.0, 2.0, 3.0])\n",
        "print(v)\n",
        "# [1.0,2.0,3.0]\n",
        "print(type(v))\n",
        "# <class 'pyspark.ml.linalg.DenseVector'>\n",
        "\n",
        "\n",
        "***\n",
        "df.rdd로 바꾸면 DataFrame의 각 Row가 파이썬 튜플 비슷한 구조로 바뀜.\n",
        "row = (0.0, 5.1, 3.5, 1.4, 0.2)  # RDD의 한 행\n",
        "\n",
        "row[0]     # 0.0 → species_indx (label)\n",
        "row[1:]    # (5.1, 3.5, 1.4, 0.2) → features\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "9JCx561xGx0I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "ab3b74e1-9eef-4978-9deb-82af2e01aea8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n***\\n🔎 DenseVector란?\\nSpark ML에서 쓰이는 특수한 벡터 자료구조.\\n내부적으로 double 배열 기반.\\n보통 features 컬럼이 DenseVector 형태여야 모델에 넣을 수 있어요\\n\\n\\n***\\nfrom pyspark.ml.linalg import DenseVector\\n\\nv = DenseVector([1.0, 2.0, 3.0])\\nprint(v)\\n# [1.0,2.0,3.0]\\nprint(type(v))\\n# <class 'pyspark.ml.linalg.DenseVector'>\\n\\n\\n***\\ndf.rdd로 바꾸면 DataFrame의 각 Row가 파이썬 튜플 비슷한 구조로 바뀜.\\nrow = (0.0, 5.1, 3.5, 1.4, 0.2)  # RDD의 한 행\\n\\nrow[0]     # 0.0 → species_indx (label)\\nrow[1:]    # (5.1, 3.5, 1.4, 0.2) → features\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: Observe the definition of the Dense Vector.\n",
        "\n",
        "\n",
        "So,when we create a new indexed dataframe(below) the machine understands\n",
        "\n",
        "\n",
        "that the first column is a Label (Target) and the remaining columns are Features."
      ],
      "metadata": {
        "id": "NaVfBJLnvoOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a new Indexed DataFrame\n",
        "df_indx = spark.createDataFrame(input_data, ['label', 'features'])"
      ],
      "metadata": {
        "id": "5U1g9VHivptU"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_indx.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4u8n-yr8xIAz",
        "outputId": "4ada5fe3-93ce-408b-e157-3e7cc6f3fd4b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----------------+\n",
            "|label|         features|\n",
            "+-----+-----------------+\n",
            "|  0.0|[5.1,3.5,1.4,0.2]|\n",
            "|  0.0|[4.9,3.0,1.4,0.2]|\n",
            "|  0.0|[4.7,3.2,1.3,0.2]|\n",
            "|  0.0|[4.6,3.1,1.5,0.2]|\n",
            "|  0.0|[5.0,3.6,1.4,0.2]|\n",
            "+-----+-----------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Scaling\n"
      ],
      "metadata": {
        "id": "cXCaBBGhxMdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Standard Scaler\n",
        "stdScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n",
        "\n",
        "# Fit and transform the data\n",
        "scaler = stdScaler.fit(df_indx)\n",
        "\n",
        "# Transform the dataframe\n",
        "df_scaled = scaler.transform(df_indx)"
      ],
      "metadata": {
        "id": "xxbALxDOyo4q"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_scaled.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ESjsuezANBy",
        "outputId": "6781f136-2da5-442a-faed-6d5aacff6f53"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----------------+--------------------+\n",
            "|label|         features|     features_scaled|\n",
            "+-----+-----------------+--------------------+\n",
            "|  0.0|[5.1,3.5,1.4,0.2]|[6.15892840883878...|\n",
            "|  0.0|[4.9,3.0,1.4,0.2]|[5.9174018045706,...|\n",
            "|  0.0|[4.7,3.2,1.3,0.2]|[5.67587520030241...|\n",
            "|  0.0|[4.6,3.1,1.5,0.2]|[5.55511189816831...|\n",
            "|  0.0|[5.0,3.6,1.4,0.2]|[6.03816510670469...|\n",
            "+-----+-----------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping the Features column\n",
        "df_scaled = df_scaled.drop('features')"
      ],
      "metadata": {
        "id": "vG8pedU5AW1w"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_scaled.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UTmtltvAoHR",
        "outputId": "17240408-fe24-4d9c-9461-491d93047df6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+\n",
            "|label|     features_scaled|\n",
            "+-----+--------------------+\n",
            "|  0.0|[6.15892840883878...|\n",
            "|  0.0|[5.9174018045706,...|\n",
            "|  0.0|[5.67587520030241...|\n",
            "|  0.0|[5.55511189816831...|\n",
            "|  0.0|[6.03816510670469...|\n",
            "+-----+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Split\n",
        "---\n",
        "Just like always, before building a model we shall split our scaled dataset into training & test sets.\n",
        "\n",
        " Training Dataset = 90% Test Dataset = 10%"
      ],
      "metadata": {
        "id": "I-v6zqyyApZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = df_scaled.randomSplit([0.9, 0.1], seed = 12345)"
      ],
      "metadata": {
        "id": "J3eZT3ScA7Wc"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GgGV9_4Bz0-",
        "outputId": "85d4e9ad-baad-4e70-e0f7-5813dc85f521"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+\n",
            "|label|     features_scaled|\n",
            "+-----+--------------------+\n",
            "|  0.0|[5.19282199176603...|\n",
            "|  0.0|[5.31358529390013...|\n",
            "|  0.0|[5.31358529390013...|\n",
            "|  0.0|[5.31358529390013...|\n",
            "|  0.0|[5.43434859603422...|\n",
            "+-----+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build, Train & Evaluate Model\n",
        "---"
      ],
      "metadata": {
        "id": "UcRF4-G3B5x3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this step we will create multiple models, train them on our scaled dataset and then compare their accuracy.\n",
        "\n"
      ],
      "metadata": {
        "id": "aA4rr5KdNFfm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ['Decision Tree', 'Random Forest', 'Naive Bayes']\n",
        "model_results = []"
      ],
      "metadata": {
        "id": "cCQxQCPmCAv-"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Decision Tree Classifier"
      ],
      "metadata": {
        "id": "-Z3idoxuZ4lv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decision Tree Classifier\n",
        "dtc = DecisionTreeClassifier(labelCol = 'label', featuresCol = 'features_scaled')\n",
        "dtc_model = dtc.fit(train_data)\n",
        "dtc_pred = dtc_model.transform(test_data)\n",
        "\n",
        "# Evaluate the Model\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol = 'label',\n",
        "                                              predictionCol = 'prediction',\n",
        "                                              metricName = 'accuracy')\n",
        "\n",
        "dtc_acc = evaluator.evaluate(dtc_pred)\n",
        "\n",
        "# print Decision Tree Classifier Accuracy = {:.2%}.format(dtc_acc)\n",
        "model_results.extend([model[0], '{:.2%}'.format(dtc_acc)])\n"
      ],
      "metadata": {
        "id": "zX9Yv_1UZ7w0"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Random Forest Classifier"
      ],
      "metadata": {
        "id": "eNJzy6F6NUtF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rfc = RandomForestClassifier(labelCol = 'label', featuresCol = 'features_scaled', numTrees= 10)\n",
        "\n",
        "rfc_model = rfc.fit(train_data)\n",
        "\n",
        "rfc_pred = rfc_model.transform(test_data)\n",
        "\n",
        "# Evaluate the Model\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol='label',\n",
        "                                              predictionCol='prediction',\n",
        "                                              metricName= 'accuracy')\n",
        "\n",
        "rfc_acc = evaluator.evaluate(rfc_pred)\n",
        "\n",
        "model_results.extend([[model[1], '{:.2%}'.format(rfc_acc)]])"
      ],
      "metadata": {
        "id": "Z5dik38ROvEm"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Naive Bayes Classifier\n"
      ],
      "metadata": {
        "id": "o96StMsrP_gS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nbc = NaiveBayes(smoothing= 1.0,\n",
        "                 modelType = 'multinomial',\n",
        "                 labelCol= 'label',\n",
        "                 featuresCol = 'features_scaled')\n",
        "\n",
        "nbc_model = nbc.fit(train_data)\n",
        "\n",
        "nbc_pred = nbc_model.transform(test_data)\n",
        "\n",
        "# Evaluate the Model\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol='label',\n",
        "                                              predictionCol='prediction',\n",
        "                                              metricName= 'accuracy')\n",
        "\n",
        "nbc_acc = evaluator.evaluate(nbc_pred)\n",
        "model_results.extend([[model[2], '{:.2%}'.format(nbc_acc)]])"
      ],
      "metadata": {
        "id": "njUb2e40POEL"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nU07ZiaIPzJI",
        "outputId": "70b84155-916b-458a-95bc-28070b655b22"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "442"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tabulate([model_results], headers = ['Model', 'Accuracy']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46eIr7LCQDOm",
        "outputId": "9a3b7ccb-604e-49de-d180-95c5f2a2b2ba"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                       Model                         Accuracy\n",
            "-------------  ------  ----------------------------  --------------------------\n",
            "Decision Tree  90.91%  ['Random Forest', '100.00%']  ['Naive Bayes', '100.00%']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j52qSHI8QKio"
      },
      "execution_count": 28,
      "outputs": []
    }
  ]
}